#!/usr/bin/env python3
# experiment_inference_comparison.py
"""
ã€å®éªŒä¸‰ï¼šæ¨ç†ç­–ç•¥å¯¹æ¯”è„šæœ¬ã€‘
ç›®çš„ï¼šåœ¨åŒä¸€å¼ é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šï¼Œä½¿ç”¨åŒä¸€ä¸ªæœ€ä¼˜æ¨¡å‹ï¼Œ
      ç›´è§‚åœ°å¯¹æ¯”â€œéé‡å æ»‘çª—â€å’Œâ€œé‡å æ»‘çª—â€ä¸¤ç§æ¨ç†ç­–ç•¥çš„è§†è§‰æ•ˆæœã€‚

åŠŸèƒ½ï¼š
 - å¯¹åŒä¸€å¼ å›¾è¿›è¡Œä¸¤æ¬¡æ¨ç†ï¼Œä¸€æ¬¡é‡å ç‡ä¸º0ï¼Œä¸€æ¬¡ä¸º0.25ã€‚
 - å°†ä¸¤æ¬¡çš„å¯è§†åŒ–ç»“æœæ‹¼æ¥åœ¨ä¸€å¼ å¯¹æ¯”å›¾ä¸Šï¼Œå¹¶æ·»åŠ æ ‡ç­¾ã€‚
 - ã€å·²ä¼˜åŒ–ã€‘æ ‡ç­¾å­—ä½“å¤§å°ä¼šæ ¹æ®å›¾åƒå®½åº¦è‡ªé€‚åº”è°ƒæ•´ï¼Œæ•ˆæœæ›´ç¾è§‚ã€‚
 - ã€å†æ¬¡ä¼˜åŒ–ã€‘å¢åŠ äº†æ ‡ç­¾æ é«˜åº¦ï¼Œä½¿æ–‡å­—ä¸å›¾åƒåˆ†éš”æ›´å¼€ï¼Œå¹¶å¾®è°ƒå­—ä½“å¤§å°ã€‚
"""
import os
import cv2
import torch
import numpy as np
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2
import segmentation_models_pytorch as smp


# -----------------------
# 1. é…ç½® (è¯·åŠ¡å¿…æ ¹æ®æ‚¨çš„ç¯å¢ƒä¿®æ”¹)
# -----------------------
class CFG:
    IMAGE_PATH = "test.png"
    MODEL_PATH = "best_model_compat.pth"  # ç¡®ä¿æŒ‡å‘ä½ æœ€å¥½çš„æ¨¡å‹
    OUTPUT_PATH = "inference_comparison_result2.png"
    WINDOW_SIZE = 512
    OVERLAP_RATIOS_TO_TEST = (0.0, 0.5)
    MODEL_ARCH = "UnetPlusPlus"
    ENCODER = "resnet101"
    CLASSES = {"background": 0, "normal_stroke": 1, "defect_area": 2}
    NUM_CLASSES = len(CLASSES)
    DEFECT_CLASS_ID = CLASSES['defect_area']
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# -----------------------
# 2. æ¨¡å‹ä¸å›¾åƒå¤„ç†å‡½æ•° (ä¸ä¹‹å‰ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
# -----------------------
def build_model(cfg):
    if cfg.MODEL_ARCH == "UnetPlusPlus":
        model = smp.UnetPlusPlus(encoder_name=cfg.ENCODER, encoder_weights=None, in_channels=3, classes=cfg.NUM_CLASSES)
    else:
        raise ValueError("MODEL_ARCH must be UnetPlusPlus for this experiment, or update build_model.")
    return model


def run_inference_on_image(model, original_image, transform, cfg, overlap_ratio):
    print(f"--- æ­£åœ¨ä»¥ {overlap_ratio * 100:.0f}% é‡å ç‡è¿›è¡Œæ¨ç† ---")
    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)
    img_h, img_w = gray_image.shape
    input_image_3ch = np.stack([gray_image] * 3, axis=-1)
    preds_full_size = np.zeros((cfg.NUM_CLASSES, img_h, img_w), dtype=np.float32)
    visits_full_size = np.zeros((img_h, img_w), dtype=np.uint8)
    stride = int(cfg.WINDOW_SIZE * (1 - overlap_ratio)) if overlap_ratio < 1 else cfg.WINDOW_SIZE
    with torch.no_grad():
        for y1 in tqdm(range(0, img_h, stride), desc=f"Overlap {overlap_ratio * 100}%"):
            y2 = min(y1 + cfg.WINDOW_SIZE, img_h)
            if y2 - y1 < cfg.WINDOW_SIZE: y1 = max(0, y2 - cfg.WINDOW_SIZE)
            for x1 in range(0, img_w, stride):
                x2 = min(x1 + cfg.WINDOW_SIZE, img_w)
                if x2 - x1 < cfg.WINDOW_SIZE: x1 = max(0, x2 - cfg.WINDOW_SIZE)
                patch = input_image_3ch[y1:y2, x1:x2]
                image_tensor = transform(image=patch)['image'].unsqueeze(0).to(cfg.DEVICE)
                with torch.amp.autocast(device_type=cfg.DEVICE.type):
                    patch_preds = model(image_tensor)
                patch_preds_cpu = patch_preds.squeeze(0).cpu().to(torch.float32).numpy()
                preds_full_size[:, y1:y2, x1:x2] += patch_preds_cpu
                visits_full_size[y1:y2, x1:x2] += 1
    avg_preds = preds_full_size / (visits_full_size + 1e-6)
    final_pred_mask = np.argmax(avg_preds, axis=0).astype(np.uint8)
    defect_mask = (final_pred_mask == cfg.DEFECT_CLASS_ID).astype(np.uint8)
    final_image = original_image.copy()
    if np.sum(defect_mask) > 0:
        red_overlay = np.zeros_like(final_image, dtype=np.uint8);
        red_overlay[:] = (0, 0, 255)
        red_mask = cv2.bitwise_and(red_overlay, red_overlay, mask=defect_mask)
        final_image = cv2.addWeighted(final_image, 1.0, red_mask, 0.6, 0)
        contours, _ = cv2.findContours(defect_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(final_image, contours, -1, (0, 255, 255), 2)
    return final_image


# -----------------------
# 3. ä¸»åè°ƒå™¨ (å¤§éƒ¨åˆ†ç›¸åŒï¼Œåªä¿®æ”¹äº†ç¬¬4éƒ¨åˆ†)
# -----------------------
def main():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£ç¼–æ’æ•´ä¸ªå¯¹æ¯”å®éªŒã€‚"""
    print("=" * 50);
    print("        å®éªŒä¸‰ï¼šæ¨ç†ç­–ç•¥å¯¹æ¯”");
    print("=" * 50)
    if not os.path.exists(CFG.IMAGE_PATH): print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•å›¾ç‰‡ '{CFG.IMAGE_PATH}'ã€‚"); return
    if not os.path.exists(CFG.MODEL_PATH): print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ '{CFG.MODEL_PATH}'ã€‚"); return

    # --- 1, 2, 3 éƒ¨åˆ†ä¸ä¹‹å‰å®Œå…¨ç›¸åŒ ---
    print(f"æ­£åœ¨ä» '{CFG.MODEL_PATH}' åŠ è½½æ¨¡å‹...")
    model = build_model(CFG)
    state_dict = torch.load(CFG.MODEL_PATH, map_location=CFG.DEVICE)
    if 'module.' in list(state_dict.keys())[0]: state_dict = {k.replace('module.', ''): v for k, v in
                                                              state_dict.items()}
    model.load_state_dict(state_dict);
    model.to(CFG.DEVICE);
    model.eval()

    original_image = cv2.imread(CFG.IMAGE_PATH)
    transform = A.Compose([A.Normalize(), ToTensorV2()])

    results_images = []
    for overlap in CFG.OVERLAP_RATIOS_TO_TEST:
        results_images.append(run_inference_on_image(model, original_image, transform, CFG, overlap))

    # --- 4. ã€å†æ¬¡ä¼˜åŒ–ã€‘åˆ›å»ºå¹¶ä¿å­˜æœ€ç»ˆçš„ã€å­—ä½“å’Œé—´è·éƒ½æ›´ä¼˜çš„å¯¹æ¯”å›¾ ---
    print("\n--- æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„ä¼˜åŒ–ç‰ˆå¯¹æ¯”å›¾ ---")

    h1, w1, _ = results_images[0].shape
    h2, w2, _ = results_images[1].shape
    if h1 != h2 or w1 != w2:
        target_h, target_w = min(h1, h2), min(w1, w2)
        results_images[0] = results_images[0][:target_h, :target_w]
        results_images[1] = results_images[1][:target_h, :target_w]

    comparison_body = cv2.hconcat(results_images)
    body_h, body_w, _ = comparison_body.shape

    # --- åŠ¨æ€è®¡ç®—å­—ä½“å’Œæ ‡ç­¾æ å‚æ•° ---
    font = cv2.FONT_HERSHEY_SIMPLEX
    color = (0, 0, 0)  # é»‘è‰²å­—ä½“

    text1 = f"Non-Overlapping (Overlap = {CFG.OVERLAP_RATIOS_TO_TEST[0] * 100:.0f}%)"
    available_width_for_text = body_w / 2

    # ã€ä¿®æ”¹ã€‘ç›®æ ‡æ¯”ä¾‹ä»0.8æé«˜åˆ°0.9ï¼Œè®©å­—ä½“æ›´å¤§
    target_text_width = available_width_for_text * 0.9

    (base_text_width, base_text_height), _ = cv2.getTextSize(text1, font, 1, 1)
    font_scale = target_text_width / base_text_width
    font_thickness = max(1, int(font_scale * 1.5))  # ç²—ç»†ä¹Ÿéšä¹‹è°ƒæ•´

    (final_text_width, final_text_height), _ = cv2.getTextSize(text1, font, font_scale, font_thickness)

    # ã€ä¿®æ”¹ã€‘å¢åŠ æ ‡ç­¾æ é«˜åº¦çš„è¾¹è·ï¼Œä»ä¸Šä¸‹å„20(æ€»å…±40)å¢åŠ åˆ°ä¸Šä¸‹å„40(æ€»å…±80)
    header_h = final_text_height + 80
    header = np.ones((header_h, body_w, 3), dtype=np.uint8) * 255

    # --- åœ¨æ ‡ç­¾æ ä¸Šç»˜åˆ¶æ–‡å­— ---
    pos1_x = int((available_width_for_text / 2) - (final_text_width / 2))
    pos1_y = int((header_h + final_text_height) / 2)
    cv2.putText(header, text1, (pos1_x, pos1_y), font, font_scale, color, font_thickness, cv2.LINE_AA)

    text2 = f"Overlapping (Overlap = {CFG.OVERLAP_RATIOS_TO_TEST[1] * 100:.0f}%)"
    (text2_width, _), _ = cv2.getTextSize(text2, font, font_scale, font_thickness)
    pos2_x = int(available_width_for_text + (available_width_for_text / 2) - (text2_width / 2))
    pos2_y = int((header_h + final_text_height) / 2)
    cv2.putText(header, text2, (pos2_x, pos2_y), font, font_scale, color, font_thickness, cv2.LINE_AA)

    final_comparison_image = cv2.vconcat([header, comparison_body])
    cv2.imwrite(CFG.OUTPUT_PATH, final_comparison_image)

    print("=" * 50);
    print(f"ğŸ‰ å®éªŒå®Œæˆï¼ä¼˜åŒ–ç‰ˆå¯¹æ¯”å›¾å·²ä¿å­˜è‡³ '{CFG.OUTPUT_PATH}'");
    print("=" * 50)


if __name__ == '__main__':
    main()