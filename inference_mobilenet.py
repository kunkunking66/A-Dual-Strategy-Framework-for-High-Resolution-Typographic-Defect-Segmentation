import torch
import cv2
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
import segmentation_models_pytorch as smp
import os


# --- 1. é…ç½® (Configuration) ---
class INFERENCE_CFG:
    # --- ã€éœ€è¦æ‚¨ä¿®æ”¹ã€‘ ---
    # æŒ‡å‘æ‚¨çš„é«˜åˆ†è¾¨ç‡ç°åº¦æ¸¬è©¦åœ–
    IMAGE_PATH = "test_image_high_res.png"

    # ã€é‡è¦ä¿®æ”¹1ã€‘: æŒ‡å‘æ‚¨ç”¨ MobileNetV2 è¨“ç·´å‡ºçš„æ¨¡å‹æ¬Šé‡
    MODEL_PATH = "best_model_defect_iou.pth"

    # æŒ‡å®šä¸€å€‹æ–°çš„è¼¸å‡ºæ–‡ä»¶åï¼Œä»¥å€åˆ†çµæœ
    OUTPUT_PATH = "result_from_mobilenet.png"

    # --- ã€é‡è¦ä¿®æ”¹2ã€‘: å¿…é ˆèˆ‡æ‚¨ç¬¬ä¸€å€‹æ¨¡å‹çš„è¨“ç·´é…ç½®å®Œå…¨ä¸€è‡´ ---
    # æ‚¨ç¬¬ä¸€å€‹æ¨¡å‹ä½¿ç”¨çš„éª¨å¹¹æ˜¯ mobilenet_v2
    ENCODER = 'mobilenet_v2'

    # --- ã€ä»¥ä¸‹éƒ¨åˆ†ä¿æŒä¸è®Šã€‘ ---
    CLASSES = {'background': 0, 'normal_stroke': 1, 'defect_area': 2}
    NUM_CLASSES = len(CLASSES)
    DEFECT_CLASS_ID = CLASSES['defect_area']

    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    INFERENCE_SIZE = 768


def predict_and_visualize(cfg):
    """åŠ è¼‰æ¨¡å‹ä¸¦å°å–®å¼µé«˜åˆ†è¾¨ç‡ç°åº¦åœ–é€²è¡Œæ¨ç†å’Œå¯è¦–åŒ–"""
    print(f"ä½¿ç”¨è¨­å‚™: {cfg.DEVICE}")
    print(f"========== æ­£åœ¨æ¸¬è©¦ MobileNetV2 æ¨¡å‹ ==========")

    # --- æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ---
    if not os.path.exists(cfg.IMAGE_PATH):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è¼¸å…¥åœ–ç‰‡ '{cfg.IMAGE_PATH}'")
        return
    if not os.path.exists(cfg.MODEL_PATH):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ '{cfg.MODEL_PATH}'")
        return

    # --- 1. åŠ è¼‰æ¨¡å‹ (æ¶æ§‹å¿…é ˆåŒ¹é…) ---
    print(f"æ­£åœ¨å¾ '{cfg.MODEL_PATH}' åŠ è¼‰æ¨¡å‹...")
    model = smp.Unet(
        encoder_name=cfg.ENCODER,  # ä½¿ç”¨ mobilenet_v2 æ¶æ§‹
        encoder_weights=None,
        in_channels=3,
        classes=cfg.NUM_CLASSES,
    )
    model.load_state_dict(torch.load(cfg.MODEL_PATH, map_location=cfg.DEVICE))
    model.to(cfg.DEVICE)
    model.eval()

    # --- 2. åœ–åƒé è™•ç† (ä¿æŒä¸è®Š) ---
    transform = A.Compose([
        A.Resize(cfg.INFERENCE_SIZE, cfg.INFERENCE_SIZE),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2(),
    ])

    original_image_for_viz = cv2.imread(cfg.IMAGE_PATH)
    original_h, original_w, _ = original_image_for_viz.shape

    gray_image_for_model = cv2.imread(cfg.IMAGE_PATH, cv2.IMREAD_GRAYSCALE)
    input_image = np.stack([gray_image_for_model] * 3, axis=-1)

    augmented = transform(image=input_image)
    image_tensor = augmented['image'].unsqueeze(0).to(cfg.DEVICE)

    print("ğŸ§  æ­£åœ¨åŸ·è¡Œæ¨ç†...")
    # --- 3. åŸ·è¡Œæ¨ç† (ä¿æŒä¸è®Š) ---
    with torch.no_grad():
        predictions = model(image_tensor)

    # --- 4. å¾Œè™•ç†å’Œå¯è¦–åŒ– (ä¿æŒä¸è®Š) ---
    pred_mask_resized = torch.argmax(predictions.squeeze(0), dim=0).cpu().numpy()
    defect_mask_resized = (pred_mask_resized == cfg.DEFECT_CLASS_ID).astype(np.uint8)
    defect_mask_original_size = cv2.resize(
        defect_mask_resized, (original_w, original_h), interpolation=cv2.INTER_NEAREST
    )

    if np.sum(defect_mask_original_size) == 0:
        print("âœ… åœ¨åœ–ç‰‡ä¸­æœªæª¢æ¸¬åˆ°ä»»ä½•ç¼ºé™·ã€‚")
        cv2.imwrite(cfg.OUTPUT_PATH, original_image_for_viz)
        print(f"çµæœå·²ä¿å­˜è‡³ '{cfg.OUTPUT_PATH}'")
        return

    print("ğŸ¨ æ­£åœ¨æ¨™è¨˜æª¢æ¸¬åˆ°çš„ç¼ºé™·å€åŸŸ...")
    red_overlay = np.zeros_like(original_image_for_viz, dtype=np.uint8);
    red_overlay[:] = (0, 0, 255)
    red_mask = cv2.bitwise_and(red_overlay, red_overlay, mask=defect_mask_original_size)
    final_image = cv2.addWeighted(original_image_for_viz, 1.0, red_mask, 0.6, 0)
    contours, _ = cv2.findContours(defect_mask_original_size, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(final_image, contours, -1, (0, 255, 255), 2)

    # --- 5. ä¿å­˜çµæœ ---
    cv2.imwrite(cfg.OUTPUT_PATH, final_image)
    print(f"ğŸ‰ æ¨ç†å®Œæˆï¼çµæœå·²ä¿å­˜è‡³ '{cfg.OUTPUT_PATH}'")


if __name__ == '__main__':
    predict_and_visualize(INFERENCE_CFG)